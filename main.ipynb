{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7ab9f1",
   "metadata": {},
   "source": [
    "# `model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d01c08",
   "metadata": {},
   "source": [
    "## (1) Character encoding + padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba513d",
   "metadata": {},
   "source": [
    "Map each character \"c\" in the input string to an integer id i (e.g. \"1\" is mapped to 2; \"0\" is mapped to 1; \" \" is mapped to 11; \"?\" is mapped to 15). \n",
    "\n",
    "This leaves that 0 can be used for padding only.\n",
    "\n",
    "This is a discrete representation of the input string for the model to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "468d3f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2,  1,  4, 11, 12, 11,  5,  1, 11, 14, 11, 15])\n",
      "tensor([ 6,  1, 11, 12, 11,  8, 11, 14, 11, 15])\n"
     ]
    }
   ],
   "source": [
    "from src.model import encode\n",
    "s1 = \"103 + 40 = ?\"\n",
    "s2 = \"50 + 7 = ?\"\n",
    "print(encode(s1))\n",
    "print(encode(s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4f41c",
   "metadata": {},
   "source": [
    "Batches need a common length $T$, so we right-pad short sequences with `pad_id=0`.\n",
    "\n",
    "The boolean `mask` marks which positions are **real tokens** vs **padding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c103562e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  1,  4, 11, 12, 11,  5,  1, 11, 14, 11, 15],\n",
      "        [ 6,  1, 11, 12, 11,  8, 11, 14, 11, 15,  0,  0]])\n",
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False]])\n"
     ]
    }
   ],
   "source": [
    "from src.model import pad_batch\n",
    "xs = [encode(s1), encode(s2)]\n",
    "xs_padded, mask = pad_batch(xs, pad_id=0)\n",
    "print(xs_padded)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd036c3b",
   "metadata": {},
   "source": [
    "## (2) Dataset for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c507f",
   "metadata": {},
   "source": [
    "TextDataset: \n",
    "\n",
    "digest `.tsv` file and return:\n",
    "\n",
    "`inputs = encode(\"x + y = ?\"), targets = float(z)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add415a",
   "metadata": {},
   "source": [
    "## (3) TinyTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ebe844",
   "metadata": {},
   "source": [
    "**Embeddings + positions**\n",
    "\n",
    "`h = self.tok(x) + self.pos(pos)`\n",
    "- `tok` is an embedding matrix $E \\in \\mathbb{R}^{V \\times d}$. For token id $x_t$, \n",
    "\\begin{equation}\n",
    "e_t = E[x_t]\n",
    "\\end{equation}\n",
    "\n",
    "- `pos` is a learned positional embedding $P \\in \\mathbb{R}^{T_{max} \\times d}$. For position $t$, \n",
    "\\begin{equation}\n",
    "p_t = P[t] \\in \\mathbb{R}^d\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1914eb",
   "metadata": {},
   "source": [
    "**Transformer encoder layer (per layer $l$)**\n",
    "\n",
    "Each layer does self‑attention + MLP, with residuals and layer norms (PyTorch’s TransformerEncoderLayer handles the exact pre/post‑norm order):\n",
    "\n",
    "1. **Self-attension:**\n",
    "\n",
    "- Project to queries, keys and values:\n",
    "\\begin{equation}\n",
    "Q=HW_Q , \\quad K=HW_K, \\quad V=HW_V, \\quad H \\in \\mathbb{R}^{T \\times d}.\n",
    "\\end{equation}\n",
    "\n",
    "- Attention scores:\n",
    "\\begin{equation}\n",
    "A=\\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} + \\text{mask} \\right)\n",
    "\\end{equation}\n",
    "where $\\text{mask}$ sets $-\\infty$ on padded positions so they get zero attention.\n",
    "\n",
    "- Context:\n",
    "\\begin{equation}\n",
    "\\text{Attention}(H)=AV\n",
    "\\end{equation}\n",
    "\n",
    "- Multi-head: repeat in parallel over $H$ heads, then concat and linearly mix.\n",
    "\n",
    "2. **Feed-forward (MLP):**\n",
    "\\begin{equation}\n",
    "MLP(h)=W_2 GeLU(W_1 h + b_1) + b_2\n",
    "\\end{equation}\n",
    "\n",
    "3. **Pooling and regression**\n",
    "- Get the last token vector in $\\mathbb{R}^d$: `pooled = h[:, -1, :]`\n",
    "- Final prediction is linear in that representation:\n",
    "\\begin{equation}\n",
    "y = w^T h_T^{(L)} + b\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49118cc",
   "metadata": {},
   "source": [
    "## (4) Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75f4df3",
   "metadata": {},
   "source": [
    "`S = 2000.0`\n",
    "\n",
    "`y = (true_sum) / S`\n",
    "\n",
    "Raw sums are $\\approx 0 - 2000$. Scaling makes targets $\\in [0,1]$, stabilizing optimization (step sizes, gradients)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25ee53c",
   "metadata": {},
   "source": [
    "**Loss**\n",
    "\n",
    "For a batch with batch size $B$,\n",
    "\\begin{equation}\n",
    "L = \\frac{1}{B} \\sum_{i=1}^B (\\hat{y}_i - y_i)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f0e792",
   "metadata": {},
   "source": [
    "## (5) Padding mask\n",
    "\n",
    "`kpm = ~(x != 0)`\n",
    "\n",
    "`h = enc(h, src_key_padding_mask=kpm)`\n",
    "\n",
    "This tells attention to ignore positions where token id equals 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c06e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhimeiliu/Downloads/DIS/dis_sheet/lib/python3.12/site-packages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "Epoch 1| train loss: 0.9369 val loss: 0.0921\n",
      "Epoch 2| train loss: 0.0550 val loss: 0.0243\n",
      "Epoch 3| train loss: 0.0268 val loss: 0.0127\n",
      "Epoch 4| train loss: 0.0149 val loss: 0.0036\n",
      "Epoch 5| train loss: 0.0093 val loss: 0.0018\n",
      "Epoch 6| train loss: 0.0066 val loss: 0.0015\n",
      "Epoch 7| train loss: 0.0064 val loss: 0.0012\n",
      "Epoch 8| train loss: 0.0061 val loss: 0.0013\n",
      "Epoch 9| train loss: 0.0053 val loss: 0.0015\n",
      "Epoch 10| train loss: 0.0054 val loss: 0.0028\n",
      "Best val MSE: 0.001234409105964005\n"
     ]
    }
   ],
   "source": [
    "# Train the tiny transformer on the REG-SUM task\n",
    "!python3 -m src.train_transformer --task REG-SUM --epochs 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71dc57bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhimeiliu/Documents/code/mechanistic_interpretability/src/plot_reports.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt, map_location=device))\n",
      "/Users/zhimeiliu/Downloads/DIS/dis_sheet/lib/python3.12/site-packages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    }
   ],
   "source": [
    "!python -m src.plot_reports --task REG-SUM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c807258",
   "metadata": {},
   "source": [
    "# `run_probes.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f96e93",
   "metadata": {},
   "source": [
    "**Question:** How linearly recoverable is the true sum $s=x+y$ from the model's hidden states?\n",
    "\n",
    "**Idea:**\n",
    "\n",
    "- Run the trained transformer on the validation set to get hidden states $h \\in \\mathbb{R}^{B \\times T \\times D}$.\n",
    "- Build targets $s_i = x_i + y_i$ for each example $i$.\n",
    "- For a chosen representation $H$ (e.g. the last token's hidden state, shape $B \\times D$), fit a linear ridge regressor $\\hat{s} = H w + b$.\n",
    "- Report $R^2$ on the same data. High $R^2 \\implies$ the sum is a linear feature in that representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d94abbf",
   "metadata": {},
   "source": [
    "Model to train probe:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{w,b} || Hw + b -s||^2_2 + \\alpha ||w||^2_2\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd2b773",
   "metadata": {},
   "source": [
    "## What a linear probe does?\n",
    "\n",
    "Suppose we have hidden states $h_i \\in \\mathbb{R}^D$ from our transformer, one per example $i$. We also have labels $s_i=x_i + y_i$ (the true sum).\n",
    "\n",
    "A linear probe fits:\n",
    "\\begin{equation}\n",
    "\\hat{s}_i = w^T h_i + b\n",
    "\\end{equation}\n",
    "\n",
    "This is just linear regression (ridge if adding L2 regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a10aa2",
   "metadata": {},
   "source": [
    "The $R^2$ score is:\n",
    "\\begin{equation}\n",
    "R^2 = 1 - \\frac{\\sum_i (s_i - \\hat{s}_i)^2}{\\sum_i (s_i - \\bar{s})^2}\n",
    "\\end{equation}\n",
    "- Numerator: residual error of the probe.\n",
    "- Denominator: variance of the true sums.\n",
    "- $R^2$ measures the fraction of variance in the true sum values that can be explained by a linear function of the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b953423f",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "\n",
    "- If $R^2 \\approx 1.0$: the sum is linearly encoded in the hidden states. That means the information “what is $x+y$?” exists as a nearly linear direction in the space.\n",
    "- If $R^2 \\approx 0$:  the hidden state has no better linear correlation with the sum than just predicting the mean. The information may still be there, but in a non-linear form the probe can’t extract.\n",
    "\n",
    "So probe $R^2$ is not about whether the model itself predicts well — it’s about whether you, as an outside observer, can read off the sum linearly from the internal activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29105a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhimeiliu/Documents/code/mechanistic_interpretability/src/run_probes.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt, map_location=device))\n",
      "/Users/zhimeiliu/Downloads/DIS/dis_sheet/lib/python3.12/site-packages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "Saved probe results to analysis/REG-SUM_probe_results.json\n",
      "{'pooled_r2': 0.9862602949142456, 'pos_r2': [0.8318772912025452, 0.8069847822189331, 0.9027222990989685, 0.7592655420303345, 0.9220019578933716, 0.8171101808547974, 0.9003023505210876, 0.8351649045944214, 0.8447633981704712, 0.7930974960327148, 0.7784875631332397, 0.8305479288101196, 0.9070967435836792]}\n"
     ]
    }
   ],
   "source": [
    "!python3 -m src.run_probes --task REG-SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aadd231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to analysis/REG-SUM_probe_curve.png\n"
     ]
    }
   ],
   "source": [
    "# visualise probe results\n",
    "!python3 -m src.plot_probes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a4daf",
   "metadata": {},
   "source": [
    "# `run_patching.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c536d19",
   "metadata": {},
   "source": [
    "**Question**: If we remove the \"sum direction\" from the representation, does performance drop? If yes, the direction is not just correlated with the sum (probe $R^2$) -- it is causally important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e460e",
   "metadata": {},
   "source": [
    "**Pipeline**\n",
    "\n",
    "1. Load val data (e.g. REG-SUM_val.tsv), encode, pad\n",
    "2. Load the trained model (e.g. REG-SUM_best.pt)\n",
    "3. Forward pass with `return_h=True` and get hidden states $h \\in \\mathbb{R}^{B \\times T \\times D}$\n",
    "4. Build targets `sums = x + y` by parsing the raw strings (regex).\n",
    "5. Fit a probe on the pooled hidden states (last real token) to predict `sums`. The probe’s weight vector is the sum direction $v \\in \\mathbb{R}^D$.\n",
    "6. Project out the component of each pooled vector along $v$:\n",
    "\\begin{equation}\n",
    "    h' = h - (h \\cdot \\hat{v}) \\hat{v} , \\quad \\hat{v}=\\frac{v}{||v||}\n",
    "\\end{equation}\n",
    "7. Measure task performance before/after by training a simple ridge readout from the pooled vectors to the ground‑truth target `y_true` (the TSV value) and comparing MSE:\n",
    "    - `ridge_head_mse_before` (before)\n",
    "    - `ridge_head_mse_after` (after)\n",
    "    - `delta_mse = after − before`\n",
    "    \n",
    "If `delta_mse >> 0`, that’s causal evidence the sum direction matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf9f41d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhimeiliu/Documents/code/mechanistic_interpretability/src/run_patching.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt, map_location=device))\n",
      "/Users/zhimeiliu/Downloads/DIS/dis_sheet/lib/python3.12/site-packages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "Saved to analysis/REG-SUM_patch_results.json\n",
      "{'ridge_head_mse_before': 2440.16650390625, 'ridge_head_mse_after': 9711.5869140625, 'delta_mse': 7271.42041015625}\n"
     ]
    }
   ],
   "source": [
    "!python3 -m src.run_patching --task REG-SUM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis_sheet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
